{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_date_range_by_chunking(large_csv):\n",
    "    \"\"\"\n",
    "    In this function, the idea is to use pandas chunk feature.\n",
    "    :param large_csv: Full path to activity_log_raw.csv\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # ======================================\n",
    "    # EXPLORE THE DATA\n",
    "    # ======================================\n",
    "    # Read the first 100,000 rows in the dataset\n",
    "    df_first_100k = pd.read_csv(large_csv, nrows=100000)    \n",
    "    print(df_first_100k)\n",
    "    # Identify the time column in the dataset\n",
    "    str_time_col = 'ACTIVITY_TIME' \n",
    "\n",
    "    # ============================================================\n",
    "    # FIND THE FIRST [EARLIEST] AND LAST DATE IN THE WHOLE DATASET\n",
    "    # BY USING CHUNKING\n",
    "    # =============================================================\n",
    "    # set chunk size\n",
    "    chunksize = 1000000 \n",
    "    # declare a list to hold the dates\n",
    "    dates = []      \n",
    "    with pd.read_csv(large_csv, chunksize=chunksize) as reader: \n",
    "        for chunk in reader:\n",
    "            # convert the string to Python datetime object\n",
    "            # add a new column to hold this datetime object\n",
    "            time_col = 'activ_time' \n",
    "            chunk[time_col] = chunk[str_time_col].apply(lambda x: pd.to_datetime(x[:9]))\n",
    "            chunk.sort_values(by=time_col, inplace=True)\n",
    "            top_date = chunk.iloc[0][time_col]\n",
    "            dates.append(top_date)  \n",
    "            chunk.sort_values(by=time_col, ascending=False, inplace=True)\n",
    "            bottom_date = chunk.iloc[0][time_col]\n",
    "            dates.append(bottom_date)   \n",
    "\n",
    "\n",
    "    # Find the earliest and last date by sorting the dates list\n",
    "    sorted_dates = sorted(dates, reverse=False)     \n",
    "    first = sorted_dates[0] \n",
    "    last = sorted_dates[-1] \n",
    "    print(\"First date is {} and the last date is {}\".format(first, last))\n",
    "\n",
    "    return first, last\n",
    "\n",
    "\n",
    "def quadratic_func(x, a):\n",
    "    \"\"\"\n",
    "    Define the quadratic function: y = 2x^2 + y-1\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y = 2*(x**2) + a-1      \n",
    "    return y\n",
    "\n",
    "\n",
    "def run_the_quad_func_without_multiprocessing(list_x, list_y):\n",
    "    \"\"\"\n",
    "    Run the quadratic function on a huge list of X and Ys without using parallelism\n",
    "    :param list_x: List of xs\n",
    "    :param list_y: List of ys\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    results = [quadratic_func(x, y) for x, y in zip(list_x, list_y)]\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_the_quad_func_with_multiprocessing(list_x, list_y, num_processors):\n",
    "    \"\"\"\n",
    "    Run the quadratic function with multiprocessing\n",
    "    :param list_x: List of xs\n",
    "    :param list_y: List of xs\n",
    "    :param num_processors: Number of processors to use\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    processors = Pool(num_processors)      \n",
    "    params = [i for i in zip(list_x, list_y)]\n",
    "    results = processors.starmap(quadratic_func, params)    # POINTS: 3\n",
    "    processors.close()\n",
    "    return results\n",
    "\n",
    "\n",
    "def multiprocessing_vs_sequential_quadratic(list_len, out_plot, out_csv):\n",
    "    \"\"\"\n",
    "    Compare how\n",
    "    :param list_len:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    for i in range(1, list_len):\n",
    "        list_length = 10 ** i\n",
    "        x = [i for i in range(list_len)]       \n",
    "        y = [i for i in range(list_len)]       \n",
    "\n",
    "        start_time = datetime.now()\n",
    "        run_the_quad_func_without_multiprocessing(x, y)     \n",
    "        end_time = datetime.now()\n",
    "        time_taken_seq = (end_time - start_time).total_seconds()\n",
    "        data.append({'ListLen': list_length, 'Type' : 'Parallel', 'TimeTaken': time_taken_seq})\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        run_the_quad_func_with_multiprocessing(x, y, 4)    \n",
    "        end_time = datetime.now()\n",
    "        time_taken_mult = (end_time - start_time).total_seconds()\n",
    "        data.append({'ListLen': list_length, 'Type' : 'Sequential', 'TimeTaken': time_taken_mult})\n",
    "\n",
    "    df = pd.DataFrame(data)     \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.lineplot(data=df, x='ListLen', y='TimeTaken', hue='Type')\n",
    "    plt.savefig(out_plot)     \n",
    "    df.to_csv(out_csv, index=False)   \n",
    "\n",
    "\n",
    "def get_num_uniq_users(csv_file, userid_col):\n",
    "    \"\"\"\n",
    "    A Helper function to help get the number of unique users\n",
    "    :param csv_file: path to CSV file\n",
    "    :param userid_col: Column for user ID\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)     \n",
    "    num = df[userid_col].nunique()      \n",
    "\n",
    "    return num\n",
    "\n",
    "\n",
    "def get_tot_uniq_users_parallel(path_to_csv, num_processors):\n",
    "    \"\"\"\n",
    "\n",
    "    :param path_to_csv:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # ==================================================\n",
    "    # GET LIST OF ALL CSV FILES AND PUT IN A LIST\n",
    "    # ===================================================\n",
    "    # convert the string URL for path to a Path object for easier interaction\n",
    "    start = datetime.now()\n",
    "    path_to_csv = Path(path_to_csv)\n",
    "    list_csv = [f for f in path_to_csv.iterdir() if f.suffix == '.csv']\n",
    "\n",
    "\n",
    "    # ======================================================\n",
    "    # USE MULTIPROCESSING TO GET UNIQUE USERS FROM ALL CSV'S\n",
    "    # ======================================================\n",
    "    # Create processors\n",
    "    processors = Pool(num_processors)      \n",
    "    # Prepare parameters for the get_num_uniq_users() function\n",
    "    user_id_col = ['user_id']*len(list_csv)     # List containing parameters for the user_id column\n",
    "    params = [i for i in zip(list_csv, user_id_col)]  # combine the two lists\n",
    "    # Run the function in parallel\n",
    "    results = processors.starmap(get_num_uniq_users, params)    \n",
    "    processors.close()\n",
    "\n",
    "    # combine results\n",
    "    tot_users = sum(results)        \n",
    "    end = datetime.now()\n",
    "    time_taken = round((end - start).total_seconds(), 2)\n",
    "    print('Total unique users: {:,} in {} seconds'.format(tot_users, time_taken))\n",
    "\n",
    "    return tot_users\n",
    "\n",
    "\n",
    "def get_tot_uniq_users_seq(path_to_csv, userid_col):\n",
    "    \"\"\"\n",
    "\n",
    "    :param path_to_csv:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # ==================================================\n",
    "    # GET LIST OF ALL CSV FILES AND PUT IN A LIST\n",
    "    # ===================================================\n",
    "    # convert the string URL for path to a Path object for easier interaction\n",
    "    start = datetime.now()\n",
    "    path_to_csv = Path(path_to_csv)\n",
    "    list_csv = [f for f in path_to_csv.iterdir() if f.suffix == '.csv']\n",
    "\n",
    "    tot_users = 0\n",
    "    for csv in list_csv:\n",
    "        df = pd.read_csv(csv)       \n",
    "        uniq = df[userid_col].nunique()\n",
    "        tot_users += uniq       \n",
    "\n",
    "    end = datetime.now()\n",
    "    time_taken = round((end - start).total_seconds(), 2)\n",
    "    print('Total unique users: {:,} in {} seconds'.format(tot_users, time_taken))\n",
    "\n",
    "    return tot_users\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Question-1: Pandas chunks\n",
    "    file = \"/Volumes/GoogleDrive/My Drive/TEACHING/aims-cameroon-2020/DAY-4-data/data/activity_log_raw.csv\"\n",
    "    # get_date_range_by_chunking(file)\n",
    "\n",
    "    # Question-2: CPU bound parallelization\n",
    "    # multiprocessing_vs_sequential_quadratic(9, 'tmp.png', 'tmp.csv')\n",
    "\n",
    "    # Question-3: CPU bound parallelization with pandas\n",
    "    fpath = \"/Volumes/GoogleDrive/My Drive/TEACHING/AIMS-22/data/fragmented_CSV\"\n",
    "    get_tot_uniq_users_parallel(fpath, 12)\n",
    "    # get_tot_uniq_users_seq(fpath, 'user_id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
